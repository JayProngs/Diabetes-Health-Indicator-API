---
title: "Modelling And Performance Comparison"
author: "Jay Thakur"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

# Introduction

In this analysis, we aim to develop predictive models to classify individuals as diabetic or non-diabetic based on various health indicators from the **Diabetes Health Indicators Dataset**. Using the **tidymodels** framework in R, we will build and evaluate two types of models:

- **Classification Tree**
- **Random Forest**

Our objective is to select the best-performing model based on the **log-loss** metric, using 5-fold cross-validation on the training data. We will then compare the selected models on a test dataset to determine the overall winner.

# Data Preparation

We begin by loading the necessary libraries and importing the dataset using a relative path.

## Loading Libraries

```{r}
library(tidyverse)
library(tidymodels)
library(rpart.plot)
```

## Data preprocessing

```{r}
data <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv")

# Convert variables to factors with labels
data <- data %>%
  mutate(
    Diabetes_binary = factor(Diabetes_binary, labels = c("No", "Yes")),
    HighBP = factor(HighBP, labels = c("No", "Yes")),
    HighChol = factor(HighChol, labels = c("No", "Yes")),
    CholCheck = factor(CholCheck, labels = c("No", "Yes")),
    Smoker = factor(Smoker, labels = c("No", "Yes")),
    Stroke = factor(Stroke, labels = c("No", "Yes")),
    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, labels = c("No", "Yes")),
    PhysActivity = factor(PhysActivity, labels = c("No", "Yes")),
    Fruits = factor(Fruits, labels = c("No", "Yes")),
    Veggies = factor(Veggies, labels = c("No", "Yes")),
    HvyAlcoholConsump = factor(HvyAlcoholConsump, labels = c("No", "Yes")),
    AnyHealthcare = factor(AnyHealthcare, labels = c("No", "Yes")),
    NoDocbcCost = factor(NoDocbcCost, labels = c("No", "Yes")),
    DiffWalk = factor(DiffWalk, labels = c("No", "Yes")),
    Sex = factor(Sex, labels = c("Female", "Male"))
  ) %>%
  mutate(
    GenHlth = factor(GenHlth, ordered = TRUE, levels = c(1, 2, 3, 4, 5),
                     labels = c("Excellent", "Very Good", "Good", "Fair", "Poor")),
    Age = factor(Age, ordered = TRUE, levels = 1:13,
                 labels = c("18–24", "25–29", "30–34", "35–39", "40–44", "45–49", "50–54",
                            "55–59", "60–64", "65–69", "70–74", "75–79", "80 or older")),
    Education = factor(Education, ordered = TRUE, levels = 1:6,
                       labels = c("No schooling", "Elementary", "Some high school",
                                  "High school graduate", "Some college", "College graduate")),
    Income = factor(Income, ordered = TRUE, levels = 1:8,
                    labels = c("<$10,000", "$10,000–$15,000", "$15,000–$20,000",
                               "$20,000–$25,000", "$25,000–$35,000", "$35,000–$50,000",
                               "$50,000–$75,000", "≥$75,000"))
  )
```

## Data Splitting

We will be splitting data in 70/30 proportion for training and testing.
```{r}
set.seed(123)

# Splitting the dataset
data_split <- initial_split(data, prop = 0.7, strata = Diabetes_binary)
train_data <- training(data_split)
test_data <- testing(data_split)
```

## Classification Tree Model
I am using five predictors, BMI, Age, Physical activity, High BP and General Health.

```{r}
tree_spec <- decision_tree(
  cost_complexity = tune(),
  tree_depth = NULL,
  min_n = NULL
) %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_recipe <- recipe(Diabetes_binary ~ BMI + Age + HighBP + PhysActivity + GenHlth, data = train_data) %>%
  step_dummy(all_nominal_predictors())
```

### Cross validsation
Using 5 fold cross validation.
```{r}
tree_folds <- vfold_cv(train_data, v = 5, strata = Diabetes_binary)

tree_workflow <- workflow() %>%
  add_model(tree_spec) %>%
  add_recipe(tree_recipe)

tree_grid <- grid_regular(cost_complexity(range = c(-4, -1)), levels = 10)
```

### Training And Tuning

```{r}
tree_metrics <- metric_set(mn_log_loss)
tree_tuned <- tune_grid(
  tree_workflow,
  resamples = tree_folds,
  grid = tree_grid,
  metrics = tree_metrics
)
```

### Results
Selecting best tree for dataset.
```{r}
tree_results <- tree_tuned %>%
  collect_metrics()

best_tree <- tree_tuned %>%
  select_best(metric = "mn_log_loss")

best_tree
```

### Final Model
Fitting complete training data on selected model.
```{r}
final_tree_workflow <- tree_workflow %>%
  finalize_workflow(best_tree)

final_tree_fit <- final_tree_workflow %>%
  fit(data = train_data)
```

### Tree Visualization
Below is the tree:
```{r}
# Plot the decision tree
# str(final_tree_fit)
rpart.plot(extract_fit_parsnip(final_tree_fit)$fit)
```

## Random Forest Model
On same training dataset, I will be using random forest model.
```{r}
rf_spec <- rand_forest(
  mtry = tune(),
  trees = 500,
  min_n = NULL
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")
```

```{r}
rf_workflow <- workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(tree_recipe)

rf_grid <- grid_regular(mtry(range = c(1, 5)), levels = 5)

rf_tuned <- tune_grid(
  rf_workflow,
  resamples = tree_folds,
  grid = rf_grid,
  metrics = tree_metrics
)
```

Selecting best hyperparameter with least log loss value.
```{r}
rf_results <- rf_tuned %>%
  collect_metrics()

# Find the best hyperparameter
best_rf <- rf_tuned %>%
  select_best(metric = "mn_log_loss")

best_rf
```

### Final RF model
Fitting training data on best random forest model.
```{r}
final_rf_workflow <- rf_workflow %>%
  finalize_workflow(best_rf)

# Fit the final model
final_rf_fit <- final_rf_workflow %>%
  fit(data = train_data)

```

### Feature Importance
Plotting variable importance graph.
```{r}
rf_importance <- final_rf_fit %>%
  extract_fit_parsnip() %>%
  vip::vip()

# Plot variable importance
rf_importance
```

## Model Evaluation on Test Data

Below we will be calculating log loss for Diabetes=Yes probability on both Classification tree and Random forest model.

### Classification Tree

```{r}
tree_probs <- predict(final_tree_fit, test_data, type = "prob")

tree_results <- tree_probs %>%
  bind_cols(truth = test_data$Diabetes_binary)
tree_results <- tree_results %>%
  mutate(truth = factor(truth, levels = c("No", "Yes")))

# Calculate log-loss using only the probabilities for the 'Yes' class
tree_log_loss <- mn_log_loss(tree_results, truth = truth, .pred_Yes, event_level = "second")

tree_log_loss
```

### Random Forest

```{r}
rf_probs <- predict(final_rf_fit, test_data, type = "prob")

rf_results <- rf_probs %>%
  bind_cols(truth = test_data$Diabetes_binary)
rf_results <- rf_results %>%
  mutate(truth = factor(truth, levels = c("No", "Yes")))

# Calculate log-loss using only the probabilities for the 'Yes' class
rf_log_loss <- mn_log_loss(rf_results, truth = truth, .pred_Yes, event_level = "second")
rf_log_loss
```

### Comparison
Performance Comparison Between Classification tree and Random Forest Model:
```{r}
model_performance <- tibble(
  Model = c("Classification Tree", "Random Forest"),
  Log_Loss = c(tree_log_loss$.estimate, rf_log_loss$.estimate)
)

model_performance
```

## Conclusion

Based on log-loss metric calculation, Random forest model outperforms the classification tree model on the test dataset. This model will be selected as the final model for deployment.